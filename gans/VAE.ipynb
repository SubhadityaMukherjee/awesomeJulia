{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling DifferentialEquations [0c46a032-eb83-5123-abaf-570d42b7fbaa]\n",
      "└ @ Base loading.jl:1260\n"
     ]
    }
   ],
   "source": [
    "using Base.Iterators: partition\n",
    "using Flux\n",
    "using Flux.Optimise: update!\n",
    "using Flux: logitbinarycrossentropy\n",
    "using Images\n",
    "using MLDatasets\n",
    "using Statistics\n",
    "using Parameters: @with_kw\n",
    "using Printf\n",
    "using Random\n",
    "using DifferentialEquations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_kw struct HyperParams\n",
    "    batch_size::Int = 128\n",
    "    latent_dim::Int = 100\n",
    "    epochs::Int = 20\n",
    "    verbose_freq::Int = 1000\n",
    "    output_x::Int = 6\n",
    "    output_y::Int = 6\n",
    "    lr_dscr::Float64 = 0.0002\n",
    "    lr_gen::Float64 = 0.0002\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_output_image(gen, fixed_noise, hparams)\n",
    "    @eval Flux.istraining() = false\n",
    "    fake_images = @. cpu(gen(fixed_noise))\n",
    "    @eval Flux.istraining() = true\n",
    "    image_array = permutedims(dropdims(reduce(vcat, reduce.(hcat, partition(fake_images, hparams.output_y))); dims=(3, 4)), (2, 1))\n",
    "    image_array = @. Gray(image_array + 1f0) / 2f0\n",
    "    return image_array\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Discriminator()\n",
    "    return Chain(\n",
    "            Conv((4, 4), 1 => 64; stride = 2, pad = 1),\n",
    "            x->leakyrelu.(x, 0.2f0),\n",
    "            Dropout(0.25),\n",
    "            Conv((4, 4), 64 => 128; stride = 2, pad = 1),\n",
    "            x->leakyrelu.(x, 0.2f0),\n",
    "            Dropout(0.25), \n",
    "            x->reshape(x, 7 * 7 * 128, :),\n",
    "            Dense(7 * 7 * 128, 1))\t\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Generator()\n",
    "    return Chain(\n",
    "            Dense(hparams.latent_dim, 7 * 7 * 256),\n",
    "            BatchNorm(7 * 7 * 256, relu),\n",
    "            x->reshape(x, 7, 7, 256, :),\n",
    "            ConvTranspose((5, 5), 256 => 128; stride = 1, pad = 2),\n",
    "            BatchNorm(128, relu),\n",
    "            ConvTranspose((4, 4), 128 => 64; stride = 2, pad = 1),\n",
    "            BatchNorm(64, relu),\n",
    "            ConvTranspose((4, 4), 64 => 1, tanh; stride = 2, pad = 1),\n",
    "            )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "function discriminator_loss(real_output, fake_output)\n",
    "    real_loss = mean(logitbinarycrossentropy.(real_output, 1f0))\n",
    "    fake_loss = mean(logitbinarycrossentropy.(fake_output, 0f0))\n",
    "    return real_loss + fake_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_loss(fake_output) = mean(logitbinarycrossentropy.(fake_output, 1f0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_discriminator!(gen, dscr, x, opt_dscr, hparams)\n",
    "    noise = randn!(similar(x, (hparams.latent_dim, hparams.batch_size))) \n",
    "    fake_input = gen(noise)\n",
    "    ps = Flux.params(dscr)\n",
    "    # Taking gradient\n",
    "    loss, back = Flux.pullback(ps) do\n",
    "        discriminator_loss(dscr(x), dscr(fake_input))\n",
    "    end\n",
    "    grad = back(1f0)\n",
    "    update!(opt_dscr, ps, grad)\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_generator!(gen, dscr, x, opt_gen, hparams)\n",
    "    noise = randn!(similar(x, (hparams.latent_dim, hparams.batch_size))) \n",
    "    ps = Flux.params(gen)\n",
    "    # Taking gradient\n",
    "    loss, back = Flux.pullback(ps) do\n",
    "        generator_loss(dscr(gen(noise)))\n",
    "    end\n",
    "    grad = back(1f0)\n",
    "    update!(opt_gen, ps, grad)\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train(; kws...)\n",
    "    # Model Parameters\n",
    "    hparams = HyperParams(; kws...)\n",
    "\n",
    "    # Load MNIST dataset\n",
    "    images, _ = MLDatasets.MNIST.traindata(Float32)\n",
    "    # Normalize to [-1, 1]\n",
    "    image_tensor = reshape(@.(2f0 * images - 1f0), 28, 28, 1, :)\n",
    "    # Partition into batches\n",
    "    data = [image_tensor[:, :, :, r] |> gpu for r in partition(1:60000, hparams.batch_size)]\n",
    "\n",
    "    fixed_noise = [randn(hparams.latent_dim, 1) |> gpu for _=1:hparams.output_x*hparams.output_y]\n",
    "\n",
    "    # Discriminator\n",
    "    dscr = Discriminator() |> gpu\n",
    "\n",
    "    # Generator\n",
    "    gen =  Generator() |> gpu\n",
    "\n",
    "    # Optimizers\n",
    "    opt_dscr = ADAM(hparams.lr_dscr)\n",
    "    opt_gen = ADAM(hparams.lr_gen)\n",
    "\n",
    "    # Training\n",
    "    train_steps = 0\n",
    "    for ep in 1:hparams.epochs\n",
    "        @info \"Epoch $ep\"\n",
    "        for x in data\n",
    "            # Update discriminator and generator\n",
    "            loss_dscr = train_discriminator!(gen, dscr, x, opt_dscr, hparams)\n",
    "            loss_gen = train_generator!(gen, dscr, x, opt_gen, hparams)\n",
    "\n",
    "            if train_steps % hparams.verbose_freq == 0\n",
    "                @info(\"Train step $(train_steps), Discriminator loss = $(loss_dscr), Generator loss = $(loss_gen)\")\n",
    "                # Save generated fake image\n",
    "                output_image = create_output_image(gen, fixed_noise, hparams)\n",
    "                save(@sprintf(\"output/dcgan_steps_%06d.png\", train_steps), output_image)\n",
    "            end\n",
    "            train_steps += 1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    output_image = create_output_image(gen, fixed_noise, hparams)\n",
    "    save(@sprintf(\"output/dcgan_steps_%06d.png\", train_steps), output_image)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
